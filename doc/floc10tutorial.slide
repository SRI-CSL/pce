% -*- Mode: TeX -*-
author: S. Owre, N. Shankar
title: \textbf{Probabilistic Reasoning with PCE}
date: 
preamble: \usepackage{tabularx}
\def\comment#1{}
\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\pair}[1]{\langle #1 \rangle}
\setbeamercolor{title}{fg=white!80!blue}
\setbeamercovered{transparent}
\setbeamertemplate{bibliography item}[triangle]
\logo{\includegraphics[height=0.5cm]{sri_blue_logo}}

\comment{Abstract: Markov Logic Networks are a framework for
probabilistic inference introduced by Richardson and Domingos.  It
formalizes a graphical relational model in terms of a first-order
logic with probabilities.  These models can be used to compute the
most probable explanation (MPE) or to compute conditional and marginal
probabilities.  SRI's Probabilistic Consistency Engine (PCE) is an
implementation of the Poon and Domingos MC-SAT method of inference
which is based on a Markov Chain Monte Carlo (MCMC) analysis.  We
demonstrate the capabilities of PCE through some simple examples.  We
also outline the mathematical background behind this approach to
probabilistic inference.  }

islide: Probabilistic Inference
An acquaintance tells you she has two children, one is a boy born on
Tuesday.  What is the probability she has two boys?

Two bowls: A and B, A has 10 chocolate and 30 plain cookies, B has 20 of
each.  A bowl is picked at random, followed by a cookie picked at random,
which is plain.  Which bowl is the cookie from?

A jury is trying a case in which there are two suspects: one has a prior criminal record
and the other doesn't.  Which suspect is more  likely to be guilty?

A test for a disease has a false negative probability of $.01$, a
false positive probability of $.05$, and a typical patient has the
disease with probability $.001$\@.  What is the probability that a
patient who has tested positive have the disease?

islide: What is PCE?

PCE stands for Probabilistic Consistency Engine

It is used for probabilistic inference with Markov Logic as a formal
framework

PCE can infer the marginal probabilities of formulas based on facts
and rules.

Facts and rules are presented in an order-sorted first-order logic.

Inference is carried out using sampling-based methods in order to achieve
scale

PCE is general enough to capture other graph-based formalisms for
probabilistic reasoning

islide: Overview

Small Example

Probability Basics

Graphical Models

PCE Background

PCE MCSAT Algorithm

PCE Example: Boy Born on Tuesday

PCE Application: Machine Reading

Conclusions
\comment{
%%slide: Small Example - Markov Logic Network

%% \begin{itemize}
%% \item Smoking causes cancer
%% \item Friends influence smoking behavior
%% \end{itemize}

%% \includegraphics[width=\textwidth]{cancer}
}
slide: Small Example - PCE Specification
\begin{alltt}\smaller{
sort Person;
const A, B, C: Person;
predicate Friends(Person, Person) indirect;
predicate Smoking(Person) indirect;
predicate Cancer(Person) indirect;

#Friends of Friends are Friends
add [x, y, z] Friends(x, y) and Friends(y, z) implies Friends(x, z) 0.7;

#Friendless people smoke

# Smoking causes cancer.
add [x] Smoking(x) => Cancer(x)  1.5;

# If two people are friends, either both smoke or neither does.
add [x, y] Friends(x, y) implies (Smoking(x) implies Smoking(y))  1.1;

#Friendship commutes
add [x, y] Friends(x, y) implies Friends(y, x);

add Smoking(A);
add Friends(A, B);
#add Friends(B, C);

mcsat_params 100000, .5, 5, .05, 100, 5;
mcsat;
dumptable atom;
}\end{alltt}

slide: Small PCE Example - Results
\begin{alltt}\smaller{
--------------------------------------------------------------------------------
|  i | prob   | atom                                                            |
--------------------------------------------------------------------------------
| 0  |  0.708 | Friends(A, A)
| 1  |  1.000 | Friends(A, B)
| 2  |  0.254 | Friends(A, C)
| 3  |  1.000 | Friends(B, A)
| 4  |  0.713 | Friends(B, B)
| 5  |  0.256 | Friends(B, C)
| 6  |  0.254 | Friends(C, A)
| 7  |  0.256 | Friends(C, B)
| 8  |  0.583 | Friends(C, C)
| 9  |  1.000 | Smoking(A)
| 10 |  0.698 | Smoking(B)
| 11 |  0.506 | Smoking(C)
| 12 |  0.817 | Cancer(A)
| 13 |  0.730 | Cancer(B)
| 14 |  0.675 | Cancer(C)
--------------------------------------------------------------------------------
}\end{alltt}

islide: Probability Basics (From Neapolitan)

Given a sample space $\Omega$ of the form $\{e_1,\ldots, e_n\}$.

An event $E$ is a subset of $\Omega$\@.

A probability function $P$ assigns a value in $[0, 1]$ to events such that
\begin{enumerate}
\item $P(\{e_1\}) +\ldots + P(\{e_n\}) = 1$, and
\item $P(E) = \Sigma_{e\in E} P(\{e\})$.
\end{enumerate}

Example: For a fair 6-sided dice, the probability $P(i)$ for $1\leq i\leq
6$ is $\frac{1}{6}$, and the probability of an even number is
$\frac{1}{6} + \frac{1}{6} + \frac{1}{6} = \frac{1}{2}$

slide: Bayes Theorem (Wikipedia)

Bayes' theorem relates the conditional and marginal probabilities of
events A and B, where B has a non-vanishing probability:
\begin{center}
    $P(A|B) = {\displaystyle \frac{P(B | A)\, P(A)}{P(B)}}$.
\end{center}

Each term in Bayes' theorem has a conventional name:
\begin{itemize}
\item $P(A)$ is the prior or marginal probability of $A$.
\item $P(A|B)$ is the conditional or posterior probability of $A$, given $B$.
\item $P(B|A)$ is the conditional probability of $B$ given $A$. It is also
called the likelihood.
\item $P(B)$ is the prior or marginal probability of $B$; acts as a
normalizing constant.
\end{itemize}

islide: Logic and Probability (Wikipedia)

Medical diagnosis offers a simple example of Bayesian reasoning.

We have a test for a disease that returns positive or negative results. 

If the patient has the disease, the test is positive  with probability $.99$.

If the patient does not have the disease, the test is positive with probability $.05$.

A patient has the disease with probability $.001$.

What is the probability that a patient with a positive test has the disease?

$P(D | pos) = P(pos | D)  P(D)/P(pos) =
.99 \times .001/ (.99\times .001 + .05 \times .999) = 99/5094 = .0194$

slide: Medical Diagnosis in PCE

\begin{alltt}\smaller{
sort Patient;
const a: Patient;
predicate testedPositive(Patient) hidden;
predicate diseased(Patient) hidden;
add testedPositive(a) or ~diseased(a) 4.6; # 99%
add ~testedPositive(a) or ~diseased(a) .01; # 1%
add testedPositive(a) or diseased(a) .05;   # 5%
add ~testedPositive(a) or diseased(a) 3.0; # 95%
add ~diseased(a) 6.9; # 99.9%
add diseased(a) .001; # .1%

add testedPositive(a);
mcsat_params 1000000, 0.5, 20.0, 0.5, 30;
ask diseased(a);

\textit{Result:}
[] 0.020: (diseased(a))
}\end{alltt}

islide: Graphical Models

The general idea is that you have some joint probability distribution
over random variables $X = \{X_1,\ldots, X_n\}$

From the joint distribution, you would like to compute marginal
probabilities $P(X_i = x_i)$ and conditional probabilities $P(Z = z | Y =
y)$, where $Z$ and $Y$ are disjoint subsets of $X$

\emph{Graphical models} represent the joint distribution $P(X = x)$ as a
product $\frac{1}{Z} \Pi_k F_k(X_k = x_k)$, where $k$ is a subset of indices,
$X_k$ is the corresponding sequence of variables, $x_k$ the corresponding
sequence of values.

$F_k$ is a potential function, and $Z$ is a normalization constant
computed as $\Sigma_x P(x = x)$

\emph{Bayesian networks} and \emph{Markov Logic Networks} are both
instances of graphical models

In a Bayesian network, the joint probabilities at a node are relative to
the parent nodes, whereas a Markov logic network is undirected

islide: Bayesian Networks

Potential functions in a Bayesian Network can be converted to feature
functions using a log-linear model as $P(X = x) = \frac{1}{Z} e^{(\Sigma_i w_i
f_i(x))}$.

A Bayesian Network can be used to compute joint probability distributions
$P(x_1, \ldots, x_n)$ by taking each individual variable $X_i$, its
parents $\pi_i$ and the valuation of its parents $X_{\pi_i} = x_{\pi_i}$
as $\Pi_i P(X_i = x_i | X_{\pi_i} = x_{\pi_i})$\@.

For many inference problems, the random variables are partitioned as $X$ and $Y$. 

Bayesian networks and other graphical models can be used to compute
\begin{enumerate}
\item Marginal probabilities: $p(X = x) = \Sigma_y p(X=x, Y = y)$

\item Maximum a posteriori probability: $\mathit{argmax}_y p(X = x, Y = y)$

\item Conditional probability: $p(X = x | Y = y) \Sigma_z p(X = x, Y = y, Z= z)/p(Y = y) $
\end{enumerate}

islide: Markov Logic Networks

Markov Logic Networks were developed by Pedro Domingos et al.

More details may be found in Pedro's recently published book, available
online at \url{http://dx.doi.org/10.2200/S00206ED1V01Y200907AIM007}

In a Markov Logic Network, all random variables are Boolean and all
feature functions are Boolean formulas

To compute conditional probabilities $P(X = x | Y = y)$ over a graphical
model, one typically uses Markov Chain Monte Carlo (MCMC) sampling
algorithms to marginalize over the variables in $Z$\@.  

A Markov chain is a state machine with probabilities attached to the
transitions between states -- sum of probabilities of outgoing transitions
from each state must add up to $1$

\emph{MCSAT} is used as the MCMC sampling method for Markov logic networks

islide: PCE Background

PCE is based on Markov Logic, and uses MCSAT to compute marginal
probabilities

The input language consists of sorts, subsorts, constants, observable/direct
predicates and hidden/indirect predicates, facts, and weighted rules

The MCSAT inference averages the probabilities over a sequence of random
models generated using SampleSAT

It draws heavily on the work of Pedro Domingos et al. and the Alchemy
system

The main differences are that PCE has subsorts, and is online and can run
interactively or as an XML-RPC server

On the other hand, Alchemy is more mature and includes machine learning
components and several other features

slide: MCSAT (Poon \& Domingos 2006)

\begin{itemize}
\item MCSAT generates a sequence of models $x^{(0)}, \ldots, x^{(i)}, \ldots$
\item Hard clauses are facts (and negations, by closed-world assumption)
and clauses with maximal weight
\item \textit{M} is a set of satisfied clauses
\end{itemize}

\begin{tabbing}
MC\=SAT(\textit{clauses}, \textit{weights}, \textit{num-samples})\\
\> $x^{(0)}$ $\leftarrow$ Satisfy(hard \textit{clauses}) \hspace*{5ex} (\textcolor{red}{WalkSAT})\\
\>\textbf{fo}\=\textbf{r} \textit{i} $\leftarrow$ 1 \textbf{to} \textit{num-samples} \textbf{do}\\
\>\>  \textit{M} = $\emptyset$\\
\>\>  \textbf{fo}\=\textbf{r all} $c_k$ $\in$ \textit{clauses} satisfied by
$x^{(i-1)}$ \textbf{do}\\
\>\>\>    \textbf{wi}\=\textbf{th probability} $1 - e^{-w_k}$ add $c_k$ to
$M$\\
\>\> \textbf{end for}\\
\>\> Sample $x^{(i)} \sim U_{SAT(M)}$ (\=Random model for set $M$)\\
\>\>\> (\textcolor{red}{SampleSAT})\\
\> \textbf{end for}
\end{tabbing}

slide: WalkSAT Algorithm (Selman et al. 1996)

\begin{tabbing}
Wa\=lkSAT(\textit{clauses}, \textit{max-tries}, \textit{max-flips}, \textit{p})\\
\>\textbf{fo}\=\textbf{r} \textit{i} $\leftarrow$ 1 \textbf{to} \textit{max-tries} \textbf{do}\\
\>\>  \textit{solution} = random truth assignment\\
\>\>  \textbf{fo}\=\textbf{r} \textit{j} $\leftarrow$ 1 \textbf{to} \textit{max-flips} \textbf{do}\\
\>\>\>    \textbf{if}\= \ all \textit{clauses} satisfied \textbf{then}\\
\>\>\>\>      \textbf{return} \textit{solution}\\
\>\>\>    \textit{c} $\leftarrow$ random unsatisfied clause\\
\>\>\>    \textbf{with probability} \textit{p}\\
\>\>\>\>      flip a random variable in \textit{c}\\
\>\>\>    \textbf{else}\\
\>\>\>\>      fli\=p variable in \textit{c} that maximizes\\
\>\>\>\>\>        number of satisfied \textit{clauses}\\
\> \textbf{return} failure\\
\end{tabbing}

slide: SampleSAT (Wei et al. 2004)
\begin{tabbing}
Sa\=mpleSAT(\textit{clauses}, \textit{max-tries}, \textit{max-flips},
\textit{p}, \textcolor{red}{\textit{sa-p}, \textit{sa-temp}})\\
\>\textbf{fo}\=\textbf{r} \textit{i} $\leftarrow$ 1 \textbf{to} \textit{max-tries} \textbf{do}\\
\>\>  \textit{solution} = random truth assignment\\
\>\>  \textbf{fo}\=\textbf{r} \textit{j} $\leftarrow$ 1 \textbf{to} \textit{max-flips} \textbf{do}\\
\>\>\>    \textcolor{red}{\textbf{wi}}\=\textcolor{red}{\textbf{th
probability} $1 - \textit{sa-p}$}\\
\>\>\>\>    \textbf{if}\= \ all \textit{clauses} satisfied \textbf{then}\\
\>\>\>\>\>      \textbf{return} \textit{solution}\\
\>\>\>\>    \textit{c} $\leftarrow$ random unsatisfied clause\\
\>\>\>\>    \textbf{with probability} \textit{p}\\
\>\>\>\>\>      flip a random variable in \textit{c}\\
\>\>\>\>    \textbf{else}\\
\>\>\>\>\>      fli\=p variable in \textit{c} that maximizes\\
\>\>\>\>\>\>        number of satisfied \textit{clauses}\\
\>\>\>   \textcolor{red}{\textbf{else}}\\
\>\>\>\> \textcolor{red}{perform simulated annealing step with \textit{sa-temp}}\\
\> \textbf{return} failure, best \textit{solution} found\\
\end{tabbing}

slide: PCE Example: Boy Born on Tuesday

\begin{quote}\smaller{
An acquaintance tells you she has two children, one is a boy born on
tuesday.  What is the probability she has two boys?
}\end{quote}
\pause
\begin{alltt}\smaller\smaller{
sort Day;
sort Child;
const Mo, Tu, We, Th, Fr, Sa, Su: Day;
const A, B: Child;
predicate boy(Child) hidden;
predicate born_on(Child, Day) hidden;

# Every child must be born on one and only one day
add [c] born_on(c, Mo) or born_on(c, Tu) or born_on(c, We)
     or born_on(c, Th) or born_on(c, Fr) or born_on(c, Sa)
     or born_on(c, Su);
add [c, d1, d2] (born_on(c, d1) and born_on(c, d2)) implies d1 = d2;
add (born_on(A, Tu) and boy(A)) or (born_on(B, Tu) and boy(B));

mcsat_params 100000, .5, 5, .05, 100, 5;
ask (boy(A) and boy(B));

\textit{Result:}
[] 0.477: (boy(A)) & (boy(B))
}\end{alltt}

slide: Which bowl is the cookie from?

\begin{quote}\smaller{
Two bowls: A and B, A has 10 chocolate and 30 plain cookies, B has 20 of
each.  A bowl is picked at random, followed by a cookie picked at random,
which is plain.  Which bowl is the cookie from?
}\end{quote}
\pause
\begin{alltt}\smaller\smaller{
sort Bowl;
const A, B: Bowl;
sort Color;
const plain, choc: Color;
predicate bowl(Bowl) hidden;
predicate cookie(Color) hidden;
add ~bowl(A)  or ~bowl(B);
add ~cookie(plain) or ~cookie(choc);
add bowl(A) or bowl(B);
add bowl(A) => cookie(plain) 1.386; # 75%
add bowl(A) => cookie(choc)   .288; # 25%
add bowl(B) => cookie(plain)  .693; # 50%
add bowl(B) => cookie(choc)   .693; # 50%
add cookie(plain);
mcsat_params 200000, .5, 5, .05, 100, 5;
mcsat; dumptable atom;

\textit{Result:}
| 0 |  0.607 | bowl(A)
| 1 |  0.393 | bowl(B)
}
\end{alltt}
islide: PCE Application: Machine Reading

PCE is currently being used in a machine-reading project managed by SRI

FAUST (Flexible Acquisition and Understanding System for Text)

Large project involving SRI, Stanford, PARC, UMass, UIUC, and UWashington

PCE  acts as a ``harness'', accepting weighted assertions and rules
from various classifiers and learners

slide: PCE Application: NFL Articles

An early use case for FAUST is a corpus of NFL newspaper articles

\begin{quote}
With Favre throwing the best-looking pinpoint passes this side of Troy
Aikman and with receiver Robert Brooks doing a great impression of Michael
Irvin and with the Packers' defense playing like, well, like themselves,
Green Bay routed Philadelphia, 39 to 13.
\end{quote}

Task is to determine which teams played, who won and who lost, and what
was the final score

islide: Related Work on MLNs

Alchemy: Markov logic networks in C++ (P. Domingos, Univ. of Washington)

Markov TheBeast: Markov logic networks in Java (S. Riedel)

ProbCog: Probabilistic Cognition for Cognitive Technical Systems\\
Markov logic networks in Python and Java (Dominik Jain)

PyMLNs: inference and learning tools for MLNs in Python (Dominik Jain)

Incerto: A Probabilistic Reasoner for the Semantic Web based on Markov Logic
(Pedro Oliveira)

islide: Future Work

Lazy MCSAT is an algorithm developed by Domingos that only creates
instances as needed

Make use of conditionals to make the conditional probability calculation
more convenient

Allow relational constraints, for example, that a predicate is functional

Extend rules to work with fully observable predicates

Allow probabilities to be given in place of weights - both are useful

Non-Boolean Random Variables

Weight and rule learning from training data

slide: Bibliography

\smaller\smaller{
\begin{thebibliography}{1}

\bibitem{MarkovLogicNetworks:2006}
Matthew Richardson and Pedro Domingos.
\newblock Markov logic networks.
\newblock {\em Mach. Learn.}, 62(1-2):107--136, 2006.

\bibitem{WalkSAT:1995}
Bart Selman, Henry Kautz, and Bram Cohen.
\newblock Local search strategies for satisfiability testing.
\newblock In {\em DIMACS Ser. in Discr. Math. and Theor. Comp. Sci.},
pages 521--532, 1995.

\bibitem{SampleSAT:2004}
Wei Wei, Jordan Erenrich, and Bart Selman.
\newblock Towards efficient sampling: exploiting random walk strategies.
\newblock In {\em AAAI'04}, pages 670--676. AAAI Press, 2004.

\bibitem{MC-SAT:2006}
Hoifung Poon and Pedro Domingos.
\newblock Sound and efficient inference with probabilistic and deterministic
  dependencies.
\newblock In {\em AAAI'06}, pages 458--463. AAAI Press, 2006.

\bibitem{LazySAT:2006}
Parag Singla and Pedro Domingos.
\newblock Memory-efficient inference in relational domains.
\newblock In {\em AAAI'06}, pages 488--493. AAAI Press, 2006.

\bibitem{LazyMCSAT:2008}
Hoifung Poon, Pedro Domingos, and Marc Sumner.
\newblock A general method for reducing the complexity of relational inference
  and its application to MCMC.
\newblock In {\em AAAI'08}, pages 1075--1080. AAAI Press, 2008.

\end{thebibliography}
}

islide: Conclusions

Probabilistic inferencing has always been important for domains with
uncertainty: planning, robotics, natural language processing, etc.

It is becoming important even for formal methods, as a part of certifying
reliability.

PCE is a robust and scalable tool for probabilistic inference

We invite you to try it out and give us feedback

PCE is open source, and will be available soon as a component of the
Personal Assistant that Learns (PAL) Framework at \url{http://pal.sri.com}\\
\vspace*{10mm}\smaller{Bruno Dutertre was involved in early design and implementation}


\comment{
slide: Monty Hall Problem
\begin{quote}\smaller{
Three doors, behind one of which is a car.  Contestant chooses door A, and
is shown another door, which is empty.  She is then offered the
opportunity to switch.  Should she?
}\end{quote}
\pause
\begin{alltt}\smaller{
sort Door;
const a, b, c: Door;
sort Switch;
const switch: Switch;
predicate car(Door) hidden;
predicate win(Switch) hidden;
add car(a) or car(b) or car(c);
add ~car(a) or ~car(b);
add ~car(b) or ~car(c);
add ~car(c) or ~car(a);
add car(a) implies ~win(switch);
add car(b) implies win(switch);
add car(c) implies win(switch);
mcsat_params 200000, .5, 5, .05, 100, 5;
ask win(switch);
\textit{Result:}
[] 0.628: (win(switch))
}\end{alltt}
}
